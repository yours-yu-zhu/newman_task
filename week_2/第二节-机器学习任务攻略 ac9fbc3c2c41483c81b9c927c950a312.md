# 第二节-机器学习任务攻略

# F**ramework of ML**

![https://pic2.zhimg.com/80/v2-7e4c53bf8b0e1867d389a0d417d72575_720w.webp](https://pic2.zhimg.com/80/v2-7e4c53bf8b0e1867d389a0d417d72575_720w.webp)

训练的过程一般需要**三个步骤**：

- 1、先写出带有未知参数的Function（函数）。

![https://pic1.zhimg.com/80/v2-212f55fcae8fd0d18a18281dfffc6184_720w.webp](https://pic1.zhimg.com/80/v2-212f55fcae8fd0d18a18281dfffc6184_720w.webp)

中，用θ来代表这个Model裡面所有的未知函数，也就是说这个Model代表的意思是：现在有一个function叫f(x),它里面面有一些未知的参数，这些未知的参数表示成θ，它的input叫做x，同时这个input叫做feature（特征)。F

- 2、确定一个名为Loss的Function。这个loss的输入就是一组参数，去判断这一组参数是好还是不好。
- 3、解决一个名为Optimization的问题。也就是寻找一个合适的θ，这个θ可以让Loss尽可能的小，最小的θ设定为θ*。

**有了θ**以后，就把它拿来用在测试集上，把Model中的参数用*θ*来取代，它的输入就是你现在的测试集。**

# **General Guide（一般性指南）**

![https://pic1.zhimg.com/80/v2-3eda7cda2725cbf001d13d70dc7519dc_720w.webp](https://pic1.zhimg.com/80/v2-3eda7cda2725cbf001d13d70dc7519dc_720w.webp)

从图的最上面开始，第一个是你今天如果你觉得模型预测的结果不满意的话，**第一件事情你要做的事情是,检查你的training data的loss**

也就是要先检查你的training data，看看你的model在training data上面有没有学起来，再去看testing的结果。如果你发现你的**training data的loss很大**，显然它**在训练集上面也没有训练好**。接下来你要分析一下，在训练集上面没有学好是什么原因。

**这边有两个可能,第一个可能是Model Bias（模型偏差），第二个是Optimization Issue（优化问题）**

## **Model bias（模型偏差 ）**

所谓的model bias意思是说，你的model太过简单。

![https://pic4.zhimg.com/80/v2-64e48c8c314f7ad7235ddefc9f210d8f_720w.webp](https://pic4.zhimg.com/80/v2-64e48c8c314f7ad7235ddefc9f210d8f_720w.webp)

举例来说，我们现在写了一个有未知parameter（参数）的function，这个未知的parameter中可以代入各种不同的数字。你代入θ¹ 得到一个function

![https://pic1.zhimg.com/80/v2-1d08ff89dc6734deeeda04dd4af9d1fc_720w.webp](https://pic1.zhimg.com/80/v2-1d08ff89dc6734deeeda04dd4af9d1fc_720w.webp)

，代入θ² 得到另一个function

![https://pic4.zhimg.com/80/v2-8b2e0fd8419b768c5ad21052ac8dc4ab_720w.webp](https://pic4.zhimg.com/80/v2-8b2e0fd8419b768c5ad21052ac8dc4ab_720w.webp)

,你把所有的function集合起来,得到一个function set（函数集）。

但是这个函数集太小了，其中没有一个Function可以让Loss变低。即可以让loss变低的function，不在你的model可以描述的范围内。在这个情况下,就算你找出了一个θ*，它是这些蓝色的function里面最好的那一个，但也无法让Loss足够低。

在这种情况下就可以重新设计一个Model，让Model具有更大的弹性。

![https://pic2.zhimg.com/80/v2-80ac22fcdcbb840e1b80441ef9b51b5d_720w.webp](https://pic2.zhimg.com/80/v2-80ac22fcdcbb840e1b80441ef9b51b5d_720w.webp)

举例来说，你可以**增加你输入的features。**本来我们输入的features只有前一天的数据，假设我们要预测接下来的观看人数的话，我们用前一天的数据不够多，那用56天前的数据，那model的弹性就比较大了。

你也可以用**Deep Learning增加更多的弹性**。所以如果你觉得你的model的弹性不够大，那你可以增加更多features，可以设一个更大的model；可以用deep lea rning，来增加model的弹性,这是第一个可以的解法。

![https://pic1.zhimg.com/80/v2-3eda7cda2725cbf001d13d70dc7519dc_720w.webp](https://pic1.zhimg.com/80/v2-3eda7cda2725cbf001d13d70dc7519dc_720w.webp)

但是在训练模型的时候，loss大并不一定就代表是model bias，你可能会遇到另外一个问题但是并不是training的时候,loss大就代表一定是model bias,你可能会遇到另外一个问题,这个问题是什麼,这个问题是**Optimization Issue**。

## **Optimization Issue（优化问题）**

我们一般用的optimization的方法一般gradient descent，这种方法有很多的问题。

举例来说 你可能会卡在local minima的地方，你没有办法找到一个真的可以让loss很低的参数，如果用图具象化的方式来表示,就像这个样子

![https://pic3.zhimg.com/80/v2-c696c11e8f524ec3aff341284c7894ae_720w.webp](https://pic3.zhimg.com/80/v2-c696c11e8f524ec3aff341284c7894ae_720w.webp)

蓝色部分是你的model可以表示的函式所形成的集合，你可以把θ代入不同的数值，形成不同的function。把所有的function通通集合在一起，得到这个蓝色的函数集。这个蓝色的se里面，确实包含了一些function，这些function它的loss是低的。

但问题是**gradient descent方法，没办法帮我们找出loss低的function**，gradient descent是解一个optimization的问题，找到θ* 然后就结束了。但是它给我的loss不够低。这一个model裡面，存在著某一个function，它的loss是够低的，gradient descent没有找到这一个function。这就好像是说 我们想**大海捞针,针确实在海里,但是我们却没有办法把针捞起来**。

## **所以集合上述两个问题，训练集的loss不够低的时候，到底是这两个问题中的哪一个？**

- 找不到一个loss低的function，到底是因為我们的model的弹性不够，也就是我们的海里面没有针。
- 还是说，我们的model的弹性已经够了，只是optimization gradient descent找到不到这个合适的参数，也就是它没办法把针捞出来

## **从比较中获得见解Gaining the insights from comparison（）**

**可以通过比较不同的模型来判断Model是否足够大。**

![https://pic3.zhimg.com/80/v2-9f80a0e7636df497ebf0c7f48b12e0a2_720w.webp](https://pic3.zhimg.com/80/v2-9f80a0e7636df497ebf0c7f48b12e0a2_720w.webp)

上图来源于一篇论文，这篇paper想测试2个networks

- 一个network有20层
- 一个network有56层

 把它们在测试集上进行测试，**横轴指的是training的过程**,即参数update的过程。随著参数的update，当然你的loss会越来越低，但是结果**20层的loss比较低,56层的loss比较高**。

 这个residual network（剩余网络）是比较早期的paper，那时候人们对于深度学习的了解还不够充分，认为上述问题的出现是因为overfitting（过度耦合），认为56层的network太深了。

实际上**这个不是overfitting并不是所有的结果不好,都叫做overfitting**。在训练集上20层的loss比较低,56层的loss比较高，**这代表56层的network,它的optimization没有做好**。

如果现在**56层的network要做到20层的network可以做到的事情**，对它来说是轻而易举的。它**只要前20层的参数,跟这个20层的network一样**，剩下36层什么事都不做，identity copy（特征复制）前一层的输出即可。56层的network一定可以做到20层的network可以做到的事情。所以20层的network有足够低的Loss，但56层的Loss反而更高是没道理的。

所以对于56层的network，如果你optimization成功的话，它的Loss应该要比20层的要更低。但在训练集上面并不是，这个不是overfitting，也不是model bias。因为56层network弹性是够的，这是因为你的**optimization不给力，optimization做得不够好**。

## **（从较浅的网络(或其他模型)开始，这更容易训练）Start from shallower networks (or other models), which are easier to train.**

如何才能知道Optimization到底有没有做好？

**看到一个你从来没有做过的问题，你可以先跑一些比较小的、比较浅的network，甚至可以用一些不是deep learning的方法**。比如说 linear model、support vector machine、support vector machine。它们可能是比较容易做Optimize（优化）的，它们出现Optimization失败的可能性比较小。也就是这些model它会竭尽全力的，在它们的能力范围之内找出一组最好的参数，它们不太可能出现失败的情况。所以你可以先train一些，比较浅的model，或者是一些比较简单的model，通过这些简单的model，可以提前知道到底可以得到什么样的loss。

## **（如果在较深的神经网络中无法在训练集上获得较小的Loss，则存在优化问题）If deeper networks do not obtain smaller loss on training data, then there is optimization issue.**

一般情况下较深的神经网络的弹性比较大，但如果其Loss却比浅的神经网络的Loss更大，就说明出现了Optimization Issue，即梯度下降做的不好。此时需要采取其他方法。

![https://pic2.zhimg.com/80/v2-422672e402c03d7d4e50ca2307dd95bd_720w.webp](https://pic2.zhimg.com/80/v2-422672e402c03d7d4e50ca2307dd95bd_720w.webp)

比如上图中观看人数预测的例子，2017年到2020年的资料是训练集，第一层的network的loss是0.28k，第二层就降到0.18k，3层就降到0.14k，4层就降到0.10k。但是5层的时候结果变成0.34k，在第五层的loss变得很大。这显然不是model bias的问题，因为第4层都可以做到0.10k了，5层应该可以做得更低。**这个是optimization的问题，是因为optimization的时候做得不好才造成这个问题。**

![https://pic2.zhimg.com/80/v2-ac07338233a9f547cdee61f56abeee4d_720w.webp](https://pic2.zhimg.com/80/v2-ac07338233a9f547cdee61f56abeee4d_720w.webp)

假设你现在经过一番努力，你已经可以让你的training data的loss变小了。那接下来你就可以来看testing data loss，如果testing data loss也小，比strong baseline还要小，那训练就结束了。（上图中红框流程）

![https://pic2.zhimg.com/80/v2-64a40140f6786a3e8caf27e13bf68489_720w.webp](https://pic2.zhimg.com/80/v2-64a40140f6786a3e8caf27e13bf68489_720w.webp)

但是如果你觉得还不够小，如果training data上面的loss小，testing data上的loss大，那你可能就是真的遇到overfitting的问题。

注意，**是training的loss小（训练集的Loss小），testing的loss大（测试集的Loss大）才有可能是overfitting。**

# **Overfitting（过拟合）**

## **产生过拟合的原因**

![https://pic3.zhimg.com/80/v2-9844f48335c526664b0da4c88a4a8ca2_720w.webp](https://pic3.zhimg.com/80/v2-9844f48335c526664b0da4c88a4a8ca2_720w.webp)

这是我们的训练集，假设根据这些训练集，某一个很差的machine learning的方法，它找出了一个无用的function。

这个无用的Function，**如果把x当做输入，这个Function就去比对这个x有没有出现在训练集里面。如果x出现在训练集里面了，就把它对应的ŷ当做输出；如果x没有出现在训练集里面，就输出一个随机的值**。

也就是说这个无用的Function只能对应输出训练集里面的数据，当输入数据不属于训练集时就输出随机数。即这个Function什么都没学到，但是它**在训练集上的Loss时0**。可是在testing data上面,它的loss会变得很大,因为**它其实什么都没有预测**。

这是一个比较极端的例子，在一般的状况下，也有可能发生类似的事情。

举例来说,假设我们输入的feature叫做x,我们输出的level叫做y,那x跟y都是一维的

![https://pic4.zhimg.com/80/v2-8347c63f46c6f7aa8d80d112bc6df82b_720w.webp](https://pic4.zhimg.com/80/v2-8347c63f46c6f7aa8d80d112bc6df82b_720w.webp)

x跟y之间的关系是上图的这个二次的曲线。这个曲线刻意用虚线来表示，是因为我们通常没有办法，直接观察到这条曲线。我们真正可以观察到的是我们的训练集,训练集你可以想像成在这条曲线上面随机sample（抽样）出来的几个点。

![https://pic4.zhimg.com/80/v2-f238698f4ffb5f3c07e38cd0a7f1f2df_720w.webp](https://pic4.zhimg.com/80/v2-f238698f4ffb5f3c07e38cd0a7f1f2df_720w.webp)

今天的模型，它的能力非常的强，它的flexibility（灵活性）、弹性很大。你只给它图中这三个点，它会知道在这三个点上面要让loss变低，所以今天的model形成的曲线会通过这三个点，但是其它没有训练集作为数据限制的地方，就会有**freestyle（自由度）**。因为它的flexibility、弹性很大，所以model可以变成各式各样的function。你没有给它资料做训练，它就会有freestyle，可以产生各式各样奇怪的结果。

![https://pic2.zhimg.com/80/v2-ef0fc76eff799df1fb98131eb269ea11_720w.webp](https://pic2.zhimg.com/80/v2-ef0fc76eff799df1fb98131eb269ea11_720w.webp)

这个时候，如果再从图中放入testing data（测试集与训练集不会一模一样，它们可能是从同一个distribution sample（分布样本）中取出来的）。testing data是橙色的点，training data是蓝色的这些点。

用蓝色的这些点找出function之后，再用橙色的点进行测试的话效果不会太好。**如果model的自由度很大的话，它可以产生非常奇怪的曲线，导致其在训练集上的结果好，但是测试集上的loss很大**。也就是产生overfitting。

## **解决过拟合问题**

### **1、一般最有效的方法就是增加训练集**

![https://pic4.zhimg.com/80/v2-60b58c987579aa36a0231abc99f016ef_720w.webp](https://pic4.zhimg.com/80/v2-60b58c987579aa36a0231abc99f016ef_720w.webp)

假设你想要做一个application（应用），你发现有overfitting的问题，那首先可以采用增加训练集来解决这个问题。

如果代表训练集的蓝色的点变多了，虽然model的弹性可能很大，但是因为你有足够的训练集可以限制住Model的freestyle。

### 2.**数据增强**

但是如果你额外的数据不够多，你可以进行**data augmentation（数据增强）**，这个方法并不使用额外资料。

![https://pic3.zhimg.com/80/v2-1faf70654f5cf2b98c525b8a2b333d9a_720w.webp](https://pic3.zhimg.com/80/v2-1faf70654f5cf2b98c525b8a2b333d9a_720w.webp)

所谓的Data augmentation就是你用一些你对于这个问题的理解，自己创造出新的资料。

举例来说在做影像辨识的时候，经常做的一个步骤是，假设你的训练集里面有某一张图片，把它左右翻转，或者是把它其中一块截出来放大等等。你**做左右翻转，你的资料就变成两倍，这个就是data augmentation。**

但是要注意data augmentation不能够随便乱做，这个augment要augment得有道理。举例来说在影像辨识裡面，你就**很少看到有人把影像上下颠倒**当作augmentation。因为这些图片都是合理的图片，你把一张照片左右翻转，并不会影响里面是什么样的东西。但如果你把它颠倒了，那就很奇怪了，这可能不是真实世界会出现的影像。如果你给机器看这种奇怪的影像的话，它可能就会学到奇怪的东西。

所以**data augmentation要根据你对资料的特性，对你现在要处理的问题的理解来选择合适的data augmentation的方式**

### 3**、限制模型的弹性**

![https://pic4.zhimg.com/80/v2-d7bcb5b1078a4f90d10cd71ccb4657e7_720w.webp](https://pic4.zhimg.com/80/v2-d7bcb5b1078a4f90d10cd71ccb4657e7_720w.webp)

举例来说，对于现在的model，我们可以用某种方式猜测出x跟y背后的关系其实就是一条二次曲线，只是我们不确定这二次曲线里面的参数。

那如何才能知道是否要对model采取限制？**要用多constrain的model才会好呢？**这就取决与你对要解决的问题的理解。因为model是你自己设计的，到底model要多constrain、多flexible，结果才会好，那这个要问你自己。

![https://pic2.zhimg.com/80/v2-0fb795c2dcebb28f97c9e6cbac7c2509_720w.webp](https://pic2.zhimg.com/80/v2-0fb795c2dcebb28f97c9e6cbac7c2509_720w.webp)

假设我们已经知道Model就是二次曲线，那你就会在选择Function的时候有很大的限制，因为已经提前限制住了Model形状的类型。所以当我们的训练集有限的时候，我们来来去去，只能够选符合二次曲线的几个function。

所以有可能在训练集只有三个点的时候，我们在有限的function中选中了与真正的distribution（分布）比较接近的function，从而可以在测试集上取得较好的效果。

所以这是第二个方法，解决overfitting问题你可以给你的model一些限制，最好你的model实现的过程正好与产生数据的过程是一样的。那你可能就会得到好的结果。

### **给Model制造限制的方法**

![https://pic2.zhimg.com/80/v2-42d5776c0460371bd974a45740f34fe9_720w.webp](https://pic2.zhimg.com/80/v2-42d5776c0460371bd974a45740f34fe9_720w.webp)

- 给它**比较少的参数**。如果是deep learning的话，就给它比较少的神经元的数目。本来每层一千个神经元，改成一百个神经元之类的；或者是你可以让model共用参数，让一些参数有一样的数值。我们之前讲的network的架构，叫做**fully-connected network（全连接网络）**，这是一个比较有弹性的架构；而**CNN（卷积神经网络）**是一个比较有限制的架构，它是针对影像的特性,来限制模型的弹性。所以fully-connected的network，可以找出来的function所形成的集合，其实是比较大的；CNN这个model所找出来的function，它形成的集合其实是比较小的，实际上都包含在fully-connected的network里面。但是就是因为CNN有比较大的限制，所以CNN在影像上反而会做得比较好。
- 另外一个就是用**比较少的features**。比如本来用三天的资料，改成用给两天的资料，结果就好了一些。
- 还有一个招数叫做**Early stopping（早停法）**。
- **Dropout（丢弃）**。这是另外一个在Deep Learning里面常用来限制模型的方法。

### **但是我们也不要给Model太多的限制，为什么不能给模型太多的限制呢？**

假设现在给Model更大的限制，规定一定是Linear的Model，一定是写成y=a+bx。那你的model它能够产生的function，就一定是一条直线

![https://pic4.zhimg.com/80/v2-136c391b0287a8e7cb3411e56b0ed2b3_720w.webp](https://pic4.zhimg.com/80/v2-136c391b0287a8e7cb3411e56b0ed2b3_720w.webp)

训练集里的三个点，不在任何一条直线上。但是你只能找到一条相比较其他直线更能接近这三点的直线。这个时候你的模型的限制就太大了，你在测试集上就不会得到好的结果。

但是这个问题不属于overfitting，又回到了**model bias**的问题。所以当前的Model在训练集上表现的不好并不是因为产生过拟合的问题，而是对Model限制太大，大到Model会产生Model Bias的问题。

所以就会发现**现在产生了一个矛盾的状况**。限制太低会产生过拟合，限制太高会产生Model Bias。

### **模型复杂程度简单介绍**

![https://pic3.zhimg.com/80/v2-94ac2c4368e0417a8a4821038c2fbaea_720w.webp](https://pic3.zhimg.com/80/v2-94ac2c4368e0417a8a4821038c2fbaea_720w.webp)

所谓的Model比较复杂就是**它包含的function比较多，它的参数比较多,这个就是一个比较复杂的model**。

对于一个比较复杂的model，如果你看它training的loss，就会发现**随著model越来越复杂，Training的loss可以越来越低**。但是在testing的时候，**当model越来越复杂，刚开始testing的loss会跟著下降，但是当复杂的程度超过某一个程度以后，Testing的loss就会突然暴增**

这是因为当你的model越来越复杂的时候，复杂到某一个程度overfitting就会出现。所以你在training的loss上面可以得到比较好的结果，在Testing的loss上面你会得到比较大的loss。

所以可以找到一个合适的复杂程度，当Model处于这个复杂程度时训练集和测试集的Loss都处于最低。

但是如何寻找？

![https://pic2.zhimg.com/80/v2-ba0df95aaf74d55e8c7a2eaff09defa5_720w.webp](https://pic2.zhimg.com/80/v2-ba0df95aaf74d55e8c7a2eaff09defa5_720w.webp)

假设我们有三个模型，它们的复杂的程度不太一样。我不知道要选哪一个模型才合适。你选太复杂的就overfitting，选太简单的有model bias的问题。

有的人会说可以把这三个模型都跑一遍，根据跑出来的结果来选择。（穷举法）

但是并不建议你这么做。

![https://pic4.zhimg.com/80/v2-22fa2e07733544d838dd9024efaaaa7f_720w.webp](https://pic4.zhimg.com/80/v2-22fa2e07733544d838dd9024efaaaa7f_720w.webp)

我们把之前举的那个极端的例子再拿出来。假设现在有一群model，这一群model不知道为什么都非常废。它们每一个model产生出来的都是一无是处的function。我们有一到一兆个model，这一到一兆个model不知道为什么learn出来的function都是一无是处的。它们会做的事情就是**训练集裡面有的资料就把它记下来,训练集没看过的,就直接output随机的结果**

那你不可能把这么多的模型统统跑一遍看它们的效果。

虽然说每一个模型在testing data上面输出的结果都是随机的，但是**你不断的随机,你总是会找到一个好的结果**。所以也许编号五六七八九的那个模型它找出来的function正好在testing data上面，就给你一个好的结果。所以你就觉得说 这个结果不错，我就**选这一个model、这个function当作我们最后上传的结果，当作我最后要用在private testing set上的结果**

但是**如果你这样做往往就会得到非常糟的结果**，因为这个model毕竟是随机的,它**恰好在public的testing set data上面得到一个好结果，但是它在private的testing set上可能仍然是随机的**。

# **Cross Validation（交叉验证）**

那到底要怎么选择model才是比较合理的呢？你要**把Training的资料分成两半,一部分叫作Training Set（训练集），一部分是Validation Set（验证集）**

![https://pic1.zhimg.com/80/v2-251f4595fe63e17f5ac99f5287544d20_720w.webp](https://pic1.zhimg.com/80/v2-251f4595fe63e17f5ac99f5287544d20_720w.webp)

在这组数据里，有90%的资料放在Training Set里面，有10%的资料会被拿来做Validation Set。你在Training Set上训练出来的模型**拿到Validation Set上去衡量它们的分数，你根据Validation Set上面的分数去挑选结果**。因为你在挑结果的时候是用Validation Set来挑你的model，所以你的public的Testing Set的结果就可以反应你的private Testing Set的结果。就不会得到说在public上面结果很好但是在private上面结果很差这样的状况。

![https://pic3.zhimg.com/80/v2-74acd095dbd56d5f6c4b72218fe0fff2_720w.webp](https://pic3.zhimg.com/80/v2-74acd095dbd56d5f6c4b72218fe0fff2_720w.webp)

当你看到public的结果以后，你就会去想要调整它。你现在弄了一堆模型，然后用Validation Set检查一下，找了一个模型放到public set上以后发现结果不好。绝大多数人都会因为得到这个不好的结果而去调整自己的Model，但是假设这一个route（途径）做太多次，你根据你的public Testing Set上的结果去调整你的model太多次，你就又有可能fit（匹配）在public Testing Set上的结果，然后在private Testing Set上面,得到差的结果。

其实最好的做法，就是用Validation loss（验证损失）最小的直接挑就好了，也就是你不要去管你的public Testing Set的结果。

但是现在面临一个问题，就是如何合理的分training set和validation set。

### **N-重交叉验证）N-fold Cross Validation（**

你可以用N-fold Cross Validation。

![https://pic3.zhimg.com/80/v2-12c4cc144718270d3cf514f25cd22b3e_720w.webp](https://pic3.zhimg.com/80/v2-12c4cc144718270d3cf514f25cd22b3e_720w.webp)

N-fold Cross Validation就是先**把你的训练集切成N等份**，在上图例子里我们切成三等份。切完以后,你拿其中**一份当作Validation Set**,**另外两份当Training Set**，然后这件事情你要**重复三次**。

也就是说，你先第一份第二份当Train，第三份当Validation；然后第一份第三份当Train，第二份当Validation；之后第一份当Validation，第二份第三份当Train。

然后接下来你会有三个模型，你不知道哪一个是好的，你就把这三个model在这三个setting（环境）下，在这三个Training跟Validation的data set上面通通跑一次。**然后把这三个模型,在这三种状况的结果都平均起来**，即把每一个模型在这三种状况的结果都平均起来,再看看谁的结果最好

如果你用这三个fold（层）得出来的结果是这个model 1最好，然后你再把model 1用在全部的Training Set上，然后训练出来的模型再用在Testing Set上面。

以上的步骤就是所谓的N-fold Cross Validation。

# **（不匹配） Mismatch**

![https://pic2.zhimg.com/80/v2-471b4f23fb2516b9fb7f60ea0ef4c8e5_720w.webp](https://pic2.zhimg.com/80/v2-471b4f23fb2516b9fb7f60ea0ef4c8e5_720w.webp)

在训练模型的过程，也可能会出现另一种形式的问题，称为mismatch。

也有人认为mismatch也算是一种Overfitting。这样也可以，这都只是名词定义的问题。但是要注意，**mismatch产生的原因与overfitting其实不一样**。一般的overfitting你可以用搜集更多的资料来克服，但是**mismatch意思是说你今天的训练集跟测试集它们的分布是不一样的**。

![https://pic2.zhimg.com/80/v2-813be2614716a06631cd6d59760fb915_720w.webp](https://pic2.zhimg.com/80/v2-813be2614716a06631cd6d59760fb915_720w.webp)

在训练集跟测试集分布是不一样的时候，你训练集再增加其实也没有帮助。